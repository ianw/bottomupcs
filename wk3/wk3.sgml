<chapter id="wk3">
  
  <title>The Operating System</title>

  <sect1>
    <title>The role of the operating system</title>

    <para>The operating system underpins the entire operation of the
    modern computer.</para>

    <sect2>
      <title>Abstraction of hardware</title>

      <para>The fundamental operation of the operating system (OS) is
      to abstract the hardware to the programmer and user.  The
      operating system provides generic interfaces to services
      provided by the underlying hardware.</para>

      <para>In a world without operating systems, every programmer
      would need to know the most intimate details of the underlying
      hardware to get anything to run.  Worse still, their programs
      would not run on other hardware, even with only slight
      differences.</para>

      <para>The operating system abstracts the hardware in many
      different ways; this will become much clearer over the following
      weeks.</para>

    </sect2>

    <sect2>
      <title>Multitasking</title>

      <para>We expect modern computers to do many different things at
      once, and we need some way to arbitrate between all the
      different programs running on the system.  It is the operating
      systems job to allow this to happen seamlessly.</para>

      <para>The operating system is responsible for <emphasis>resource
      management</emphasis>; that means things like the formatting,
      reading and writing disks and making sure one program does not
      take up all the system memory.  You have probably experienced
      when this has failed as it usually ends up with your computer
      crashing.</para>

    </sect2>

    <sect2>
      <title>Standardised Interfaces</title>

      <para>Programmers want to write programs that will run on as
      many different hardware platforms as possible.  By having
      operating system support for standardised interfaces,
      programmers can get this functionality.</para>

      <para>For example, if the function to open a file on one system
      is <computeroutput>open()</computeroutput>, on another is
      <computeroutput>open_file()</computeroutput> and on yet another
      <computeroutput>openf()</computeroutput> programmers will have
      the dual problem of having to remember what each system does and
      their programs will not work on multiple systems.</para>

      <para>The Portable Operating System Interface (POSIX) defined by
      the IEEE is a very important standard implemented by UNIX type
      operating systems.  Microsoft Windows has similar proprietary
      standards.
      </para>

    </sect2>

    <sect2>
      <title>Security</title>
      
      <para>On multi-user systems, security is very important.
      Ultimately, as the arbitrator of access to the system, the
      operating system is responsible for ensuring that only those
      with the correct permission can access resources.</para>

      <para>For example, if a file is owned by one user, another user
      should not be allowed to <computeroutput>open()</computeroutput>
      it.</para>

      <para>Operating systems are large and complex programs, and
      often security issues will be found.  Often a virus or worm will
      take advantage of these bugs; to fight them you must install
      <emphasis>patches</emphasis> or updates provided by your
      operating system vendor.</para>
    </sect2>

    <sect2>
      <title>Performance</title>

      <para>As the operating system provides so many services to the
      computer, it's performance is critical.  As an operating systems
      programmer, you must always be thinking about the performance of
      any code you write.</para>

    </sect2>

  </sect1>

  <sect1>
    <title>Operating System Organisation</title>

    <para>The operating system is roughly organised as in the figure below.</para>

    <figure>
      <title>The Operating System</title>
      <mediaobject>
	<imageobject>
	  <imagedata fileref="wk3/figures/kernel.eps" format="EPS" />
	</imageobject>
	<imageobject>
	  <imagedata fileref="wk3/figures/kernel.png" format="PNG" />
	</imageobject>
	<textobject>
	  <phrase>The organisation of the kernel.  Processes the
	  kernel is running live in <emphasis>userspace</emphasis>,
	  and the kernel talks both directly to hardware and through
	  <emphasis>drivers</emphasis>.
	  </phrase>
	</textobject>
      </mediaobject>
    </figure>

    <sect2>
      <title>The Kernel</title>
      
      <para>The kernel <emphasis>is</emphasis> the operating system.
      As the figure illustrates, the kernel communicates to hardware
      both directly and through <emphasis>drivers</emphasis>.</para>

      <para>Just as the kernel abstracts the hardware to user
      programs, drivers abstract hardware to the kernel.  For example,
      there are many different types of graphic card, each one with
      slightly different features.  As long as the kernel exports an
      API, people who have access to the specifications for the
      hardware can write drivers to implement that API.  This way the
      kernel can access many different types of hardware.</para>

      <para>The kernel is generally what we called
      <emphasis>privileged</emphasis>.  As you will learn, the
      hardware has important roles to play in running multiple tasks
      and keeping the system secure, but these rules do not apply to
      the kernel.  We know that the kernel must handle programs that
      crash (remember it is the operating systems job arbitrate
      between multiple programs running on the same system, and there
      is no guarantee that they will behave), but if any internal part
      of the operating system crashes chances are the entire system
      will become useless.</para>

      <sect3>
	<title>Monolithic v Microkernels</title>

	<para>One debate that is often comes up surrounding operating
	systems is whether the kernel should be a
	<emphasis>microkernel</emphasis> or
	<emphasis>monolithic</emphasis>.</para>

	<para>As discussed above, the kernel is always privileged,
	which means if any part of it crashes the whole system is
	likely to become unstable.  A microkernel architecture tries
	to minimise this possibility by making the kernel as small as
	possible.  This means that many parts of the system run in
	exactly the same way as programs, and if they crash the system
	will not necessarily become useless.</para>

	<para>Whilst this sounds like the most obvious idea, the
	problem comes back two main issues</para>

	<orderedlist>
	  <listitem>
	    <para>Performance is decreased.  Talking between many
	    different components can decrease performance.</para>
	  </listitem>
	  <listitem>

	    <para>It is slightly more difficult for the programmer.
	    </para>

	  </listitem>
	</orderedlist>

	<para>Both of these criticisms come because most microkernels
	are implemented with a <emphasis>message passing</emphasis>
	based system, commonly referred to as <emphasis>inter-process
	communication</emphasis> or IPC.  Communicating between
	individual components happens via discrete messages, which
	must be bundled up, sent to the other component, unbundled,
	operated upon, re-bundled up and sent back, and then unbundled
	again to get the result.</para>

	<para>This is a lot of steps for what might be a fairly simple
	request from a foreign component.  Obviously one request might
	make the other component do more requests of even more
	components, and the problem can multiply.  Slow message
	passing implementations were largely responsible for the poor
	performance of early microkernel systems, and the concepts of
	passing messages are slightly harder for progammers to program
	for.  The enhanced protection from having components run
	separately was not sufficient to overcome these hurdles in
	early microkernel systems, so they fell out of fashion.</para>

	<para>Thus most common operating systems use a
	<emphasis>monolithic</emphasis> kernel, where all components
	are integrated into the privileged kernel.  Calls between
	components are simple function calls, as all programmers are
	familar with.</para>

	<para>There is no definitive answer as to which is the best
	organisation, and it has started many arguments in both
	academic and non-academic circles.  Hopefully as you learn
	more about operating systems you will definitely be able to
	make up your own mind!</para>

      </sect3>

      <sect3>
	<title>Virtualisation</title>

	<para>Closely related to kernel is the concept of
	virtualisation of hardware.  Modern computers are very
	powerful, and often it is useful to not thing of them as one
	whole system but split a single physical computer up into
	separate "virtual" machines.  Each of these virtual machines
	looks for all intents and purposes as a completely separate
	machine, although physically they are all in the same box, in
	the same place.</para>

	<figure>
	  <title>The Operating System</title>
	  <mediaobject>
	    <imageobject>
	      <imagedata fileref="wk3/figures/virtual.eps" format="EPS" />
	    </imageobject>
	    <imageobject>
	      <imagedata fileref="wk3/figures/virtual.png" format="PNG" />
	    </imageobject>
	    <textobject>
	  <phrase>Some different virtualisation methods.</phrase>
	    </textobject>
	  </mediaobject>
	</figure>

	<para>This can be organised in many different ways.  In the
	simplest case, a small <emphasis>virtual machine monitor
	</emphasis> can run directly on the hardware and provide an
	interface to the guest operating systems running on top.  This
	VMM is often often called a hypervisor (from the word
	"supervisor")<footnote> <para>In fact, the hypervisor shares
	much in common with a micro-kernel; both strive to be small
	layers to present the hardware in a safe fashion to layers
	above it.</para></footnote>.  In fact, the operating system on
	top may have no idea that the hypervisor is even there at all,
	as the hypervisor presents what appears to be a complete
	system.  It intercepts operations between the guest operating
	system and hardware and only presents a subset of the system
	resources to each.</para>

	<para>This is often used on large machines (with many CPUs and
	much RAM) to implement <emphasis>partitioning</emphasis>.
	This means the machine can be split up into smaller virtual
	machines.  Often you can allocate more resources to running
	systems on the fly, as requirements dictate.  The hypervisors
	on many large IBM machines are actually quite complicated
	affairs, with many millions of lines of code.  It provides a
	multitude of system management services.</para>

	<para>Another option is to have the operating system aware of
	the underlying hypervisor, and request system resources
	through it.  This is sometimes referred to as
	<emphasis>paravirtualisation</emphasis> due to it's halfway
	nature.  This is similar to the way early versions of the Xen
	system works and is a compromise solution.  It hopefully
	provides better performance since the operating system is
	explicitly asking for system resources from the hypervisor
	when required, rather than the hypervisor having to work
	things out dynamically.</para>

	<para>Finally, you may have a situation where an application
	running on top of the existing operating system presents a
	virtualised system (including CPU, memory, BIOS, disk, etc)
	which a plain operating system can run on.  The application
	converts the requests to hardware through to the underlying
	hardware via the existing operating system.  This is similar
	to how VMWare works.  This approach has many overheads, as the
	application process has to emulate an entire system and
	convert everything to requests from the underlying operating
	system.  However, this lets you emulate an entirely different
	architecture all together, as you can dynamically translate
	the instructions from one processor type to another (as the
	Rosetta system does with Apple software which moved from the
	PowerPC processor to Intel based processors).</para>

	<para>Performance is major concern when using any of these
	virtualisation techniques, as what was once fast operations
	directly on hardware need to make their way through layers of
	abstraction.</para>

	<para>Intel have discussed hardware support for virtualisation
	soon to be coming in their latest processors.  These
	extensions work by raising a special exception for operations
	that might require the intervention of a virtual machine
	monitor.  Thus the processor looks the same as a
	non-virtualised processor to the application running on it,
	but when that application makes requests for resources that
	might be shared between other guest operating systems the
	virtual machine monitor can be invoked.</para>

	<para>This provides superior performance because the virtual
	machine monitor does not need to monitor every operation to
	see if it is safe, but can wait until the processor notifies
	that something <emphasis>unsafe</emphasis> has
	happened.</para>

	<sect4>
	  <title>Covert Channels</title>

	  <para>This is a digression, but an interesting security flaw
	  relating to virtualised machines.  If the partitioning of
	  the system is not static, but rather
	  <emphasis>dynamic</emphasis>, there is a potential security
	  issue involved.</para>

	  <para>In a dynamic system, resources are allocated to the
	  operating systems running on top as required.  Thus if one
	  is doing particularly CPU intensive operations whilst the
	  other is waiting on data to come from disks, more of the CPU
	  power will be given to the first task.  In a static system,
	  each would get 50% an the unused portion would go to
	  waste.</para>

	  <para>Dynamic allocation actually opens up a communications
	  channel between the two operating systems.  Anywhere that
	  two states can be indicated is sufficient to communicate in
	  binary.  Imagine both systems are extremely secure, and no
	  information should be able to pass between one and the
	  other, ever.  Two people with access could collude to pass
	  information between themselves by writing two programs that
	  try to take large amounts of resources at the same
	  time.</para>

	  <para>When one takes a large amount of memory there is less
	  available for the other.  If both keep track of the maximum
	  allocations, a bit of information can be transferred.  Say
	  they make a pact to check every second if they can allocate
	  this large amount of memory.  If the target can, that is
	  considered binary 0, and if it can not (the other machine
	  has all the memory), that is considered binary 1.  A data
	  rate of one bit per second is not astounding, but
	  information is flowing.</para>

	  <para>This is called a <emphasis>covert channel</emphasis>,
	  and whilst admittedly far fetched there have been examples
	  of security breaches from such mechanisms.  It just goes to
	  show that the life of a systems programmer is never simple!</para>

	</sect4>

      </sect3>

    </sect2>

    <sect2>
      <title>Userspace</title>

      <para>We call the theoretical place where programs run by the
      user <emphasis>userspace</emphasis>.  Each program runs in
      userspace, talking to the kernel through <emphasis>system
      calls</emphasis> (discussed below).</para>

      <para>As previously discussed, userspace is
      <emphasis>unprivileged</emphasis>.  User programs can only do a
      limited range of things, and should never be able to crash other
      programs, even if they crash themselves.</para>

    </sect2>

  </sect1>


  <sect1>
    <title>System Calls</title>


    <sect2>
      <title>Overview</title>

      <para>System calls are how userspace programs interact with the
    kernel.  The general principle behind how they work is described
    below.</para>
      
    <sect3>
      <title>System call numbers</title> 

	<para>Each and every system call has a <emphasis>system call
	number</emphasis> which is known by both the userspace and the
	kernel.  For example, both know that system call number 10 is
	<computeroutput>open()</computeroutput>, system call number 11
	is <computeroutput>read()</computeroutput>, etc.</para>

	<para>The <emphasis>Application Binary Interface</emphasis>
	(ABI) is very similar to an API but rather than being for
	software is for hardware.  The API will define which register
	the system call number should be put in so the kernel can find
	it when it is asked to do the system call.</para>

    </sect3>

    <sect3>
      <title>Arguments</title>

      <para>System calls are no good without arguments; for example
      <computeroutput>open()</computeroutput> needs to tell the kernel
      exactly <emphasis>what</emphasis> file to open.  Once again the
      ABI will define which registers arguments should be put into for
      the system call.</para>
    </sect3>

    <sect3>
      <title>The trap</title>

      <para>To actually perform the system call, there needs to be
      some way to communicate to the kernel we wish to make a system
      call.  All architectures define an instruction, usually called
      <computeroutput>break</computeroutput> or something similar,
      that signals to the hardware we wish to make a system
      call.</para>

      <para>Specifically, this instruction will tell the hardware to
      modify the instruction pointer to point to the kernels system
      call handler (when the operating system sets its self up it
      tells the hardware where its system call handler lives).  So
      once the userspace calls the break instruction, it has lost
      control of the program and passed it over to the kernel.</para>

      <para>The rest of the operation is fairly straight forward.  The
      kernel looks in the predefined register for the system call
      number, and looks it up in a table to see which function it
      should call.  This function is called, does what it needs to do,
      and places it's return value into <emphasis>another</emphasis>
      register defined by the ABI as the return register.</para>

      <para>The final step is for the kernel to make a jump
      instruction back to the userspace program, so it can continue
      off where it left from.  The userpsace program gets the data it
      needs from the return register, and continues happily on it's
      way!</para>

      <para>Although the details of the process can get quite hairy,
      this is basically all their is to a system call.</para>

      </sect3>

      <sect3>
	<title>libc</title>
	
	<para>Although you can do all of the above by hand for each
	system call, system libraries usually do most of the work for
	you.  The standard library that deals with system calls on
	UNIX like systems is <computeroutput>libc</computeroutput>; we
	will learn more about it's roles in future weeks.</para>
	
      </sect3>

    </sect2>


    <sect2>
      <title>Analysing a system call</title>

      <para>As the system libraries usually deal with making systems
      call for you, we need to do some low level hacking to
      illustrate exactly how the system calls work.</para>

      <para>We will illustrate how probably the most simple system
      call, <computeroutput>getpid()</computeroutput>, works.  This
      call takes no arguments and returns the ID of the currently
      running program (or process; we'll look more at the process in
      later weeks).</para>

      <example>
	<title>getpid() example</title>
	<programlisting linenumbering="numbered" lang="C"><inlinemediaobject>
	    <imageobject>
	      <imagedata fileref="wk3/code/getpid.c" format="linespecific" />
	    </imageobject>
	  </inlinemediaobject></programlisting>
      </example>

      <para>We start by writing a small C program which we can start
      to illustrate the mechanism behind system calls.  The first
      thing to note is that there is a
      <computeroutput>syscall</computeroutput> argument provided by
      the system libraries for directly making system calls.  This
      provides an easy way for programmers to directly make systems
      calls without having to know the exact assembly language
      routines for making the call on their hardware.  So why do we
      use <computeroutput>getpid()</computeroutput> at all?  Firstly,
      it is much clearer to use a symbolic function name in your code.
      However, more importantly,
      <computeroutput>getpid()</computeroutput> may work in very
      different ways on different systems.  For example, on Linux the
      <computeroutput>getpid()</computeroutput> call can be cached, so
      if it is run twice the system library will not take the penalty
      of having to make an entire system call to find out the same
      information again.</para>

      <para>By convention under Linux, system calls numbers are
      defined in the <computeroutput>asm/unistd.h</computeroutput>
      file from the kernel source.  Being in the
      <computeroutput>asm</computeroutput> subdirectory, this is
      different for each architecture Linux runs on.  Again by
      convention, system calls numbers are given a
      <computeroutput>#define</computeroutput> name consisting of
      <computeroutput>__NR_</computeroutput>.  Thus you can see our
      code will be making the <computeroutput>getpid</computeroutput>
      system call, storing the value in
      <computeroutput>pid</computeroutput>.</para>

      <para>We will have a look at how several architectures implement
      this code under the hood.  We're going to look at real code, so
      things can get quite hairy.  But stick with it -- this is
      <emphasis>exactly</emphasis> how your system works!</para>

      <sect3>
	<title>PowerPC</title>

	<para>PowerPC is a RISC architecture common in older Apple
	computers, and the core of devices such as the latest
	version of the Xbox.</para>

	<example>
	  <title>PowerPC system call example</title>
	  <programlisting linenumbering="numbered" lang="C"><inlinemediaobject>
	      <imageobject>
		<imagedata fileref="wk3/code/unistd-powerpc.h" format="linespecific" />
	      </imageobject>
	    </inlinemediaobject></programlisting>
	</example>
	
	<para>This code snippet from the kernel header file
	<computeroutput>asm/unistd.h</computeroutput> shows how we can
	implement system calls on PowerPC.  It looks very complicated,
	but it can be broken down step by step.</para>

	<para>Firstly, jump to the end of the example where the
	<computeroutput>_syscallN</computeroutput> macros are defined.
	You can see there are many macros, each one taking
	progressively one more argument.  We'll concentrate on the
	most simple version,
	<computeroutput>_syscall0</computeroutput> to start with.  It
	only takes two arguments, the return type of the system call
	(e.g. a C <computeroutput>int</computeroutput> or
	<computeroutput>char</computeroutput>, etc) and the name of
	the system call.  For <computeroutput>getpid</computeroutput>
	this would be done as
	<computeroutput>_syscall0(int,getpid)</computeroutput>.</para>

	<para>Easy so far!  We now have to start pulling apart
	<computeroutput>__syscall_nr</computeroutput> macro.  This is
	not dissimilar to where we were before, we take the number of
	arguments as the first parameter, the type, name and then the
	actual arguments.</para>

	<para>The first step is declaring some names for registers.
	What this essentially does is says
	<computeroutput>__sc_0</computeroutput> refers to
	<computeroutput>r0</computeroutput> (i.e. register 0).  The
	compiler will usually use registers how it wants, so it is
	important we give it constraints so that it doesn't decide to
	go using register we need in some ad-hoc manner.</para>

	<para>We then call
	<computeroutput>sc_loadargs</computeroutput> with the
	interesting <computeroutput>##</computeroutput> parameter.
	That is just a <emphasis>paste</emphasis> command, which gets
	replaced by the <computeroutput>nr</computeroutput> variable.
	Thus for our example it expands to
	<computeroutput>__sc_loadargs_0(name, args);</computeroutput>.
	<computeroutput>__sc_loadargs</computeroutput> we can see
	below sets <computeroutput>__sc_0</computeroutput> to be the
	system call number; notice the paste operator again with the
	<computeroutput>__NR_</computeroutput> prefix we talked about,
	and the variable name that refers to a specific
	register.</para>

	<para>So, all this tricky looking code actually does is puts
	the system call number in register 0!  Following the code
	through, we can see that the other macros will place the
	system call arguments into <computeroutput>r3</computeroutput>
	through <computeroutput>r7</computeroutput> (you can only have
	a maximum of 5 arguments to your system call).</para>

	<para>Now we are ready to tackle the
	<computeroutput>__asm__</computeroutput> section.  What we
	have here is called <emphasis>inline assembly</emphasis>
	because it is assembler code mixed right in with source code.
	The exact syntax is a little to complicated to go into right
	here, but we can point out the important parts.</para>

	<para>Just ignore the
	<computeroutput>__volatile__</computeroutput> bit for now; it
	is telling the compiler that this code is unpredictable so it
	shouldn't try and be clever with it.  Again we'll start at the
	end and work backwards.  All the stuff after the colons is a
	way of communicating to the compiler about what the inline
	assembly is doing to the CPU registers.  The compiler needs to
	know so that it doesn't try using any of these registers in
	ways that might cause a crash.</para>

	<para>But the interesting part is the two assembly statements
	in the first argument.  The one that does all the work is the
	<computeroutput>sc</computeroutput> call.  That's all you need
	to do to make your system call!</para>

	<para>So what happens when this call is made?  Well, the
	processor is interrupted knows to transfer control to a
	specific piece of code setup at system boot time to handle
	interrupts.  There are many interrupts; system calls are just
	one.  This code will then look in register 0 to find the
	system call number; it then looks up a table and finds the
	right function to jump to to handle that system call.  This
	function receives it's arguments in registers 3 - 7.</para>

	<para>So, what happens once the system call handler runs and
	completes?  Control returns to the next instruction after the
	<computeroutput>sc</computeroutput>, in this case a
	<emphasis>memory fence</emphasis> instruction.  What this
	essentially says is "make sure everything is committed to
	memory"; remember how we talked about pipelines in the
	superscalar architecture?  This instruction ensures that
	everything we think has been written to memory actually has
	been, and isn't making it's way through a pipeline
	somewhere.</para>

	<para>Well, we're almost done!  The only thing left is to
	return the value from the system call.  We see that
	<computeroutput>__sc_ret</computeroutput> is set from r3 and
	<computeroutput>__sc_err</computeroutput> is set from r0.
	This is interesting; what are these two values all
	about?</para>

	<para>One is the return value, and one is the error value.
	Why do we need two variables?  System calls can fail, just as
	any other function.  The problem is that a system call can
	return any possible value; we can not say "a negative value
	indicates failure" since a negative value might be perfectly
	acceptable for some particular system call.</para>

	<para>So our system call function, before returning, ensures
	its result is in register r3 and any error code is in register
	r0.  We check the error code to see if the top bit is set;
	this would indicate a negative number.  If so, we set the
	global <computeroutput>errno</computeroutput> value to it
	(this is the standard variable for getting error information
	on call failure) and set the return to be
	<computeroutput>-1</computeroutput>.  Of course, if a valid
	result is received we return it directly.</para>

	<para>So our calling function should check the return value is
	not <computeroutput>-1</computeroutput>; if it is it can check
	errno to find the exact reason why the call failed.</para>

	<para>And that is an entire system call on a PowerPC!</para>

      </sect3>

      <sect3>
	<title>x86 system calls</title>

	<para>Below we have the same interface as implemented for the
	x86 processor.</para>

	<example>
	  <title>x86 system call example</title>
	  <programlisting linenumbering="numbered" lang="C"><inlinemediaobject>
	      <imageobject>
		<imagedata fileref="wk3/code/unistd-x86.h" format="linespecific" />
	      </imageobject>
	    </inlinemediaobject></programlisting>
	</example>

	<para>The x86 architecture is very different from the PowerPC
	that we looked at previously.  The x86 is classed as a CISC
	processor as opposed to the RISC PowerPC, and has dramatically
	less registers.</para>

	<para>Start by looking at the most simple
	<computeroutput>_syscall0</computeroutput> macro.  It simply
	calls the <computeroutput>int</computeroutput> instruction
	with a value of <computeroutput>0x80</computeroutput>.  This
	instruction makes the CPU raise interrupt 0x80, which will
	jump to code that handles system calls in the kernel.</para>

	<para>We can start inspecting how to pass arguments with the
	longer macros.  Notice how the PowerPC implementation cascaded
	macros downwards, adding one argument per time.  This
	implementation has slightly more copied code, but is a little
	easier to follow.</para>

	<para>x86 register names are based around letters, rather than
	the numerical based register names of PowerPC.  We can see
	from the zero argument macro that only the
	<computeroutput>A</computeroutput> register gets loaded; from
	this we can tell that the system call number is expected in
	the <computeroutput>EAX</computeroutput> register.  As we
	start loading registers in the other macros you can see the
	short names of the registers in the arguments to the
	<computeroutput>__asm__</computeroutput> call.</para>

	<para>We see something a little more interesting in
	<computeroutput>__syscall6</computeroutput>, the macro taking
	6 arguments.  Notice the <computeroutput>push</computeroutput>
	and <computeroutput>pop</computeroutput> instructions?  These
	work with the stack on x86, "pushing" a value onto the top of
	the stack in memory, and popping the value from the stack back
	into memory.  Thus in the case of having six registers we need
	to store the value of the <computeroutput>ebp</computeroutput>
	register in memory, put our argument in in (the
	<computeroutput>mov</computeroutput> instruction), make our
	system call and then restore the original value into
	<computeroutput>ebp</computeroutput>.  Here you can see the
	disadvantage of not having enough registers; stores to memory
	are expensive so the more you can avoid them, the
	better.</para>

	<para>Another thing you might notice there is nothing like the
	<emphasis>memory fence</emphasis> instruction we saw
	previously with the PowerPC.  This is because on x86 the
	effect of all instructions will be guaranteed to be visible
	when the complete.  This is easier for the compiler (and
	programmer) to program for, but offers less
	flexibility.</para>

	<para>The only thing left to contrast is the return value.  On
	the PowerPC we had two registers with return values from the
	kernel, one with the value and one with an error code.
	However on x86 we only have one return value that is passed
	into <computeroutput>__syscall_return</computeroutput>.  That
	macro casts the return value to <computeroutput>unsigned
	long</computeroutput> and compares it to an (architecture and
	kernel dependent) range of negative values that might
	represent error codes (note that the
	<computeroutput>errno</computeroutput> value is positive, so
	the negative result from the kernel is negated).  However,
	this means that system calls can not return small negative
	values, since they are indistinguishable from error codes.
	Some system calls that have this requirement, such as
	<computeroutput>getpriority()</computeroutput>, add an offset
	to their return value to force it to always be positive; it is
	up to the userspace to realise this and subtract this constant
	value to get back to the "real" value.</para>

      </sect3>

    </sect2>
  </sect1>

  <sect1>
    <title>Privileges</title>

    <sect2>
      <title>Hardware</title>

	<para>We mentioned how one of the major tasks of the operating
      system is to implement security; that is to not allow one
      application or user to interfere with any other that is running
      in the system.  This means applications should not be able to
      overwrite each others memory or files, and only access system
      resources as dictated by system policy.</para>

	<para>However, when an application is running it has exclusive
      use of the processor.  We see how this works when we examine
      processes in the next chapter.  Ensuring the application only
      accesses memory it owns is implemented by the virtual memory
      system, which we examine in the chapter after next.  The
      essential point is that the hardware is responsible for
      enforcing these rules.</para>
      
      <para>The system call interface we have examined is the
	gateway to the application getting to system resources.  By
	forcing the application to request resources through a system
	call into the kernel, the kernel can enforce rules about what
	sort of access can be provided.  For example, when an
	application makes an <computeroutput>open()</computeroutput>
	system call to open a file on disk, it will check the
	permissions of the user against the file permissions and
	allow or deny access.</para>
  

      <sect3>
	<title>Privilege Levels</title>

	<para>Hardware protection can usually be seen as a set of
	concentric rings around a core set of operations.</para>

	<figure>
	  <title>Rings</title>
	  <mediaobject>
	    <imageobject>
	      <imagedata fileref="wk3/figures/priv.eps" format="EPS" />
	    </imageobject>
	    <imageobject>
	      <imagedata fileref="wk3/figures/priv.png" format="PNG" />
	    </imageobject>
	    <textobject>
	      <phrase>Privilege levels on x86</phrase>
	    </textobject>
	  </mediaobject>
	</figure>

	<para>In the inner most ring are the most protected
	instructions; those that only the kernel should be allowed to
	call.  For example, the <computeroutput>HLT</computeroutput>
	instruction to halt the processor should not be allowed to be
	run by a user application, since it would stop the entire
	computer from working.  However, the kernel needs to be able
	to call this instruction when the computer is legitimately
	shut down.<footnote> <para>What happens when a "naughty"
	application calls that instruction anyway?  The hardware will
	usually raise an exception, which will involve jumping to a
	specified handler in the operating system similar to the
	system call handler.  The operating system will then probably
	terminate the program, usually giving the user some error
	about how the application has crashed.</para></footnote></para>

	<para>Each inner ring can access any instructions protected by
	a further out ring, but not any protected by a further in ring.
	Not all architectures have multiple levels of rings as above,
	but most will either provide for at least a "kernel" and
	"user" level.</para>

	<sect4>
	  <title>386 protection model</title>

	  <para>The 386 protection model has four rings, though most
	  operating systems (such as Linux and Windows) only use two
	  of the rings to maintain compatibility with other
	  architectures that do now allow as many discrete protection
	  levels.</para>

	  <para>386 maintains privileges by making each piece of
	  application code running in the system have a small
	  descriptor, called a <emphasis>code descriptor</emphasis>,
	  which describes, amongst other things, its privilege level.
	  When running application code makes a jump into some other
	  code outside the region described by its code descriptor,
	  the privilege level of the target is checked.  If it is
	  higher than the currently running code, the jump is
	  disallowed by the hardware (and the application will crash).
	  </para>

	</sect4>

	<sect4>
	  <title>Raising Privilege</title>

	  <para>Applications may only raise their privilege level by
	specific calls that allow it, such as the instruction to
	implement a system call.  These are usually referred to as a
	<emphasis>call gate</emphasis> because they function just as a
	physical gate; a small entry through an otherwise impenetrable
	wall.  When that instruction is called we have seen how the
	hardware completely stops the running application and hands
	control over to the kernel.  The kernel must act as a
	gatekeeper; ensuring that nothing nasty is coming through the
	gate.  This means it must check system call arguments
	carefully to make sure it will not be fooled into doing
	anything it shouldn't (if it can be, that is a security bug).
	As the kernel runs in the innermost ring, it has permissions
	to do any operation it wants; when it is finished it will
	return control back to the application which will again be
	running with it's lower privilege level.</para>
	</sect4>

	<sect4>
	  <title>Fast System Calls</title>

	  <para>One problem with traps is that they are very expensive
	  for the processor to implement.  The process of looking up
	  the tables </para>

	  <para>Modern processors have realised this overhead and
	  strive to reduce it.</para>
	</sect4>

      </sect3>

    </sect2>


  </sect1>
  
</chapter>

<!-- 
Local Variables: 
mode: sgml 
sgml-parent-document: ("../csbu.sgml" "book" "chapter") 
End: 
-->
