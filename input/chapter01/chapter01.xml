<?xml version="1.0"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude" version="5.0" xml:id="chapter01">
  <info>
    <title>Binary and Number Representation</title>
  </info>
  <section xml:id="binary">
    <info>
      <title>Binary &#x2014; the basis of computing</title>
    </info>
    <section>
      <info>
        <title>Binary Theory</title>
      </info>
      <section>
        <info>
          <title>Introduction</title>
        </info>
        <para>Binary is a base-2 number system that uses two mutually
	exclusive states to represent information.  A binary number is
	made up of elements called <emphasis>bits</emphasis> where
	each bit can be in one of the two possible states.  Generally,
	we represent them with the numerals
	<computeroutput>1</computeroutput> and
	<computeroutput>0</computeroutput>.  We also talk about them
	being true and false.  Electrically, the two states might be
	represented by high and low voltages or some form of switch
	turned on or off.</para>
        <para>We build binary numbers the same way we build numbers in
	our traditional base 10 system.  However, instead of a one's
	column, a 10's column, a 100's column (and so on) we have a
	one's column, a two's columns, a four's column, an eight's
	column, and so on, as illustrated below.
	</para>
        <table>
          <info>
            <title>Binary</title>
          </info>
          <tgroup cols="8">
            <tbody>
              <row>
                <entry>2<superscript>...</superscript></entry>
                <entry>2<superscript>6</superscript></entry>
                <entry>2<superscript>5</superscript></entry>
                <entry>2<superscript>4</superscript></entry>
                <entry>2<superscript>3</superscript></entry>
                <entry>2<superscript>2</superscript></entry>
                <entry>2<superscript>1</superscript></entry>
                <entry>2<superscript>0</superscript></entry>
              </row>
              <row>
                <entry>...</entry>
                <entry>64</entry>
                <entry>32</entry>
                <entry>16</entry>
                <entry>8</entry>
                <entry>4</entry>
                <entry>2</entry>
                <entry>1</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>For example, to represent the number 203 in base 10, we
	know we place a <computeroutput>3</computeroutput> in the
	<computeroutput>1's</computeroutput> column, a
	<computeroutput>0</computeroutput> in the
	<computeroutput>10's</computeroutput> column and a
	<computeroutput>2</computeroutput> in the
	<computeroutput>100's</computeroutput> column.  This is
	expressed with exponents in the table below.</para>
        <table>
          <info>
            <title>203 in base 10</title>
          </info>
          <tgroup cols="3">
            <tbody>
              <row>
                <entry>10<superscript>2</superscript></entry>
                <entry>10<superscript>1</superscript></entry>
                <entry>10<superscript>0</superscript></entry>
              </row>
              <row>
                <entry>2</entry>
                <entry>0</entry>
                <entry>3</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>Or, in other words, 2 &#xD7;
	10<superscript>2</superscript> + 3 &#xD7;
	10<superscript>0</superscript> = 200 + 3 =
	203.  To represent the same thing in binary, we would have the
	following table.</para>
        <table>
          <info>
            <title>203 in base 2</title>
          </info>
          <tgroup cols="8">
            <tbody>
              <row>
                <entry>2<superscript>7</superscript></entry>
                <entry>2<superscript>6</superscript></entry>
                <entry>2<superscript>5</superscript></entry>
                <entry>2<superscript>4</superscript></entry>
                <entry>2<superscript>3</superscript></entry>
                <entry>2<superscript>2</superscript></entry>
                <entry>2<superscript>1</superscript></entry>
                <entry>2<superscript>0</superscript></entry>
              </row>
              <row>
                <entry>1</entry>
                <entry>1</entry>
                <entry>0</entry>
                <entry>0</entry>
                <entry>1</entry>
                <entry>0</entry>
                <entry>1</entry>
                <entry>1</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>That equates to 2<superscript>7</superscript> +
	2<superscript>6</superscript> +
	2<superscript>3</superscript>+2<superscript>1</superscript>
	+ 2<superscript>0</superscript> = 128 + 64
	+ 8 + 2 + 1 = 203.</para>
      </section>
      <section>
        <info>
          <title>The basis of computing</title>
        </info>
        <para>You may be wondering how a simple number is the basis of
        all the amazing things a computer can do.  Believe it or not,
        it is!  The processor in your computer has a complex but
        ultimately limited set of <emphasis>instructions</emphasis> it
        can perform on values such as addition, multiplication, etc.
        Essentially, each of these instructions is assigned a number
        so that an entire program (add this to that, multiply by that,
        divide by this and so on) can be represented by a just a
        stream of numbers.  For example, if the processor knows
        operation <computeroutput>2</computeroutput> is addition, then
        <computeroutput>252</computeroutput> could mean "add 5 and 2
        and store the output somewhere".  The reality is of course
        much more complicated (see <xref linkend="chapter02"/>) but,
        in a nutshell, this is what a computer is.
        </para>
        <para>In the days of punch-cards, one could see with their
          eye the one's and zero's that make up the program stream by
          looking at the holes present on the card.  Of course this
          moved to being stored via the polarity of small magnetic
          particles rather quickly (tapes, disks) and onto the point
          today that we can carry unimaginable amounts of data in our
          pocket.</para>
        <para>Translating these numbers to something useful to humans
        is what makes a computer so useful.  For example, screens are
        made up of millions of discrete <emphasis>pixels</emphasis>,
        each too small for the human eye to distinguish but combining
        to make a complete image.  Generally each pixel has a certain
        red, green and blue component that makes up it's display
        color.  Of course, these values can be represented by numbers,
        which of course can be represented by binary!  Thus any image
        can be broken up into millions of individual dots, each dot
        represented by a <emphasis>tuple</emphasis> of three values
        representing the red, green and blue values for the pixel.
        Thus given a long string of such numbers, formatted correctly,
        the video hardware in your computer can convert those numbers
        to electrical signals to turn on and off individual pixels and
        hence display an image.
        </para>
        <para>As you read on, we will build up the entire modern
            computing environment from this basic building block;
            <emphasis>from the bottom-up</emphasis> if you
            will!</para>
      </section>
      <section>
        <info>
          <title>Bits and Bytes</title>
        </info>
        <para>As discussed above, we can essentially choose to
	represent anything by a number, which can be converted to
	binary and operated on by the computer.  For example, to
	represent all the letters of the alphabet we would need at
	least enough different combinations to represent all the lower
	case letters, the upper case letters, numbers and punctuation,
	plus a few extras.  Adding this up means we need probably
	around 80 different combinations.</para>
        <para>If we have two bits, we can represent four possible
	unique combinations (<computeroutput>00 01 10
	11</computeroutput>).  If we have three bits, we can represent
	8 different combinations.  In general, with
	<computeroutput>n</computeroutput> bits we can represent
	<computeroutput>2<superscript>n</superscript></computeroutput>
	unique combinations.</para>
        <para>8 bits gives us
	<computeroutput>2<superscript>8</superscript> =
	256</computeroutput> unique representations, more than enough
	for our alphabet combinations.  We call a group of 8 bits a
	<emphasis>byte</emphasis>.  Guess how big a C
	<computeroutput>char</computeroutput> variable is?  One
	byte.</para>
        <section>
          <info>
            <title>ASCII</title>
          </info>
          <para>Given that a byte can represent any of the values 0
	  through 255, anyone could arbitrarily make up a mapping
	  between characters and numbers.  For example, a video card
	  manufacturer could decide that
	  <computeroutput>1</computeroutput> represents
	  <computeroutput>A</computeroutput>, so when value
	  <computeroutput>1</computeroutput> is sent to the video card
	  it displays a capital 'A' on the screen.  A printer
	  manufacturer might decide for some obscure reason that
	  <computeroutput>1</computeroutput> represented a lower-case
	  'z', meaning that complex conversions would be required to
	  display and print the same thing.</para>
          <para>To avoid this happening, the <emphasis>American
	  Standard Code for Information Interchange</emphasis> or
	  ASCII was invented.  This is a <emphasis>7-bit</emphasis>
	  code, meaning there are 2<superscript>7</superscript> or
	  128 available codes.</para>
          <para>The range of codes is divided up into two major parts;
	  the non-printable and the printable.  Printable characters
	  are things like characters (upper and lower case), numbers
	  and punctuation.  Non-printable codes are for control, and
	  do things like make a carriage-return, ring the terminal
	  bell or the special <computeroutput>NULL</computeroutput>
	  code which represents nothing at all.</para>
          <para>127 unique characters is sufficient for American
	  English, but becomes very restrictive when one wants to
	  represent characters common in other languages, especially
	  Asian languages which can have many thousands of unique
	  characters.</para>
          <para>To alleviate this, modern systems are moving away from
	  ASCII to <emphasis>Unicode</emphasis>, which can use up to 4
	  bytes to represent a character, giving
	  <emphasis>much</emphasis> more room!</para>
        </section>
        <section>
          <info>
            <title>Parity</title>
          </info>
          <para>ASCII, being only a 7-bit code, leaves one bit of the
	  byte spare.  This can be used to implement
	  <emphasis>parity</emphasis> which is a simple form of error
	  checking.  Consider a computer using punch-cards for input,
	  where a hole represents 1 and no hole represents 0.  Any
	  inadvertent covering of a hole will cause an incorrect value
	  to be read, causing undefined behaviour.</para>
          <para>Parity allows a simple check of the bits of a byte to
	  ensure they were read correctly.  We can implement either
	  <emphasis>odd</emphasis> or <emphasis>even</emphasis> parity
	  by using the extra bit as a <emphasis>parity
	  bit</emphasis>.</para>
          <para>In odd parity, if the number of 1's in the 7 bits of
	  information is odd, the parity bit is set, otherwise it is
	  not set.  Even parity is the opposite; if the number of 1's
	  is even the parity bit is set to 1.</para>
          <para>In this way, the flipping of one bit will case a
	  parity error, which can be detected.</para>
          <para>XXX more about error correcting</para>
        </section>
        <section>
          <info>
            <title>16, 32 and 64 bit computers</title>
          </info>
          <para>Numbers do not fit into bytes; hopefully your bank
	  balance in dollars will need more range than can fit into
	  one byte!  Most modern architectures are <emphasis>32
	  bit</emphasis> computers.  This means they work with 4 bytes
	  at a time when processing and reading or writing to memory.
	  We refer to 4 bytes as a <emphasis>word</emphasis>; this is
	  analogous to language where letters (bits) make up words in a
	  sentence, except in computing every word has the same size!
	  The size of a C <computeroutput>int</computeroutput>
	  variable is 32 bits.  Newer architectures are 64 bits, which
	  doubles the size the processor works with (8 bytes).</para>
        </section>
        <section>
          <info>
            <title>Kilo, Mega and Giga Bytes</title>
          </info>
          <para>Computers deal with a lot of bytes; that's what makes
	  them so powerful!</para>
          <para>We need a way to talk about large numbers of bytes,
	  and a natural way is to use the "International System of
	  Units" (SI) prefixes as used in most other scientific areas.
	  So for example, kilo refers to
	  10<superscript>3</superscript> or 1000 units, as in a
	  kilogram has 1000 grams.</para>
          <para>1000 is a nice round number in base 10, but in binary
	  it is <computeroutput>1111101000</computeroutput> which is
	  not a particularly "round" number.  However, 1024 (or
	  2<superscript>10</superscript>) is
	  (<computeroutput>10000000000</computeroutput>), and happens
	  to be quite close to the base ten meaning of kilo (1000 as
	  opposed to 1024).</para>
          <para>Hence 1024 bytes became known as a
	  <emphasis>kilobyte</emphasis>.  The first mass market
	  computer was the Commodore 64, so named because it had 64
	  kilobytes of storage.</para>
          <para>Today, kilobytes of memory would be small for a wrist
	  watch, let alone a personal computer.  The next SI unit is
	  "mega" for
	  <computeroutput>10<superscript>6</superscript></computeroutput>.
	  As it happens,
	  <computeroutput>2<superscript>20</superscript></computeroutput>
	  is again close to the SI base 10 definition; 1048576 as
	  opposed to 1000000.</para>
          <para>The units keep increasing by powers of 10; each
	  time it diverges further from the base SI meaning.</para>
          <table>
            <info>
              <title>Bytes</title>
            </info>
            <tgroup cols="2">
              <tbody>
                <row>
                  <entry>2<superscript>10</superscript></entry>
                  <entry>Kilobyte</entry>
                </row>
                <row>
                  <entry>2<superscript>20</superscript></entry>
                  <entry>Megabyte</entry>
                </row>
                <row>
                  <entry>2<superscript>30</superscript></entry>
                  <entry>Gigabyte</entry>
                </row>
                <row>
                  <entry>2<superscript>40</superscript></entry>
                  <entry>Terabyte</entry>
                </row>
                <row>
                  <entry>2<superscript>50</superscript></entry>
                  <entry>Petabyte</entry>
                </row>
                <row>
                  <entry>2<superscript>60</superscript></entry>
                  <entry>Exabyte</entry>
                </row>
              </tbody>
            </tgroup>
          </table>
          <para>Therefore a 32 bit computer can address up to four
	  gigabytes of memory; the extra two bits can represent four
	  groups of <computeroutput>2<superscript>30</superscript>
	  bytes.</computeroutput>.  A 64 bit computer can address up
	  to 8 exabytes; you might be interested in working out just
	  how big a number this is!  To get a feel for how big that
	  number is, calculate how long it would take to count to
	  <computeroutput>2<superscript>64</superscript></computeroutput>
	  if you incremented once per second.</para>
        </section>
        <section>
          <info>
            <title>Kilo, Mega and Giga Bits</title>
          </info>
          <para>Apart from the confusion related to the overloading of
	  SI units between binary and base 10, capacities will often
	  be quoted in terms of <emphasis>bits</emphasis> rather than
	  bytes.</para>
          <para>Generally this happens when talking about networking
	  or storage devices; you may have noticed that your ADSL
	  connection is described as something like 1500
	  kilobits/second.  The calculation is simple; multiply by
	  1000 (for the kilo), divide by 8 to get bytes and then 1024
	  to get kilobytes (so 1500 kilobits/s=183 kilobytes
	  per second).</para>
          <para>The SI standardisation body has recognised these dual
	  uses, and has specified unique prefixes for binary usage.
	  Under the standard 1024 bytes is a
	  <computeroutput>kibibyte</computeroutput>, short for
	  <emphasis>kilo binary</emphasis> byte (shortened to KiB).
	  The other prefixes have a similar prefix (Mebibyte, for
	  example).  Tradition largely prevents use of these terms,
	  but you may seem them in some literature.</para>
        </section>
        <section>
          <info>
            <title>Conversion</title>
          </info>
          <para>The easiest way to convert between bases is to use a
	  computer, after all, that's what they're good at!  However,
	  it is often useful to know how to do conversions by
	  hand.</para>
          <para>The easiest method to convert between bases is
	  <emphasis>repeated division</emphasis>.  To convert,
	  repeatedly divide the quotient by the base, until the
	  quotient is zero, making note of the remainders at each
	  step.  Then, write the remainders in reverse, starting at
	  the bottom and appending to the right each time.  An example
	  should illustrate; since we are converting to binary we use
	  a base of 2.</para>
          <table>
            <info>
              <title>Convert 203 to binary</title>
            </info>
            <tgroup cols="4">
              <thead>
                <row>
                  <entry>Quotient</entry>
                  <entry/>
                  <entry>Remainder</entry>
                  <entry/>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>203<subscript>10</subscript> &#xF7; 2 =</entry>
                  <entry>101</entry>
                  <entry>1</entry>
                  <entry/>
                </row>
                <row>
                  <entry>101<subscript>10</subscript> &#xF7; 2 =</entry>
                  <entry>50</entry>
                  <entry>1</entry>
                  <entry>&#x2191;</entry>
                </row>
                <row>
                  <entry>50<subscript>10</subscript> &#xF7; 2 =</entry>
                  <entry>25</entry>
                  <entry>0</entry>
                  <entry>&#x2191;</entry>
                </row>
                <row>
                  <entry>25<subscript>10</subscript> &#xF7; 2 =</entry>
                  <entry>12</entry>
                  <entry>1</entry>
                  <entry>&#x2191;</entry>
                </row>
                <row>
                  <entry>12<subscript>10</subscript> &#xF7; 2 =</entry>
                  <entry>6</entry>
                  <entry>0</entry>
                  <entry>&#x2191;</entry>
                </row>
                <row>
                  <entry>6<subscript>10</subscript> &#xF7; 2 =</entry>
                  <entry>3</entry>
                  <entry>0</entry>
                  <entry>&#x2191;</entry>
                </row>
                <row>
                  <entry>3<subscript>10</subscript> &#xF7; 2 =</entry>
                  <entry>1</entry>
                  <entry>1</entry>
                  <entry>&#x2191;</entry>
                </row>
                <row>
                  <entry>1<subscript>10</subscript> &#xF7; 2 =</entry>
                  <entry>0</entry>
                  <entry>1</entry>
                  <entry>&#x2191;</entry>
                </row>
              </tbody>
            </tgroup>
          </table>
          <para>Reading from the bottom and appending to the right
	  each time gives <computeroutput>11001011</computeroutput>,
	  which we saw from the previous example was 203.</para>
        </section>
      </section>
      <section>
        <info>
          <title>Boolean Operations</title>
        </info>
        <para>George Boole was a mathematician who discovered a whole
	area of mathematics called <emphasis>Boolean
	Algebra</emphasis>.  Whilst he made his discoveries in the mid
	1800's, his mathematics are the fundamentals of all computer
	science.  Boolean algebra is a wide ranging topic, we present
	here only the bare minimum to get you started.</para>
        <para>Boolean operations simply take a particular input and
	produce a particular output following a rule.  For example,
	the simplest boolean operation,
	<computeroutput>not</computeroutput> simply inverts the value
	of the input operand.  Other operands usually take two inputs,
	and produce a single output.</para>
        <para>The fundamental Boolean operations used in computer
	science are easy to remember and listed below. We represent
	them below with <emphasis>truth tables</emphasis>; they simply
	show all possible inputs and outputs.  The term
	<emphasis>true</emphasis> simply reflects
	<computeroutput>1</computeroutput> in binary.</para>
        <section>
          <info>
            <title>Not</title>
          </info>
          <para>Usually represented by
	  <computeroutput>!</computeroutput>,
	  <computeroutput>not</computeroutput> simply inverts the
	  value, so <computeroutput>0</computeroutput> becomes
	  <computeroutput>1</computeroutput> and
	  <computeroutput>1</computeroutput> becomes
	  <computeroutput>0</computeroutput></para>
          <table>
            <info>
              <title>Truth table for <emphasis>not</emphasis></title>
            </info>
            <tgroup cols="2">
              <thead>
                <row>
                  <entry>Input</entry>
                  <entry>Output</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>
        <section>
          <info>
            <title>And</title>
          </info>
          <para>To remember how the and operation
	  works think of it as "if one input <emphasis>and</emphasis>
	  the other are true, result is true</para>
          <table>
            <info>
              <title>Truth table for <emphasis>and</emphasis></title>
            </info>
            <tgroup cols="3">
              <thead>
                <row>
                  <entry>Input 1</entry>
                  <entry>Input 2</entry>
                  <entry>Output</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>
        <section>
          <info>
            <title>Or</title>
          </info>
          <para>To remember how the
	  <computeroutput>or</computeroutput> operation works think of
	  it as "if one input <emphasis>or</emphasis> the other input
	  is true, the result is true</para>
          <table>
            <info>
              <title>Truth table for <emphasis>or</emphasis></title>
            </info>
            <tgroup cols="3">
              <thead>
                <row>
                  <entry>Input 1</entry>
                  <entry>Input 2</entry>
                  <entry>Output</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>
        <section>
          <info>
            <title>Exclusive Or (xor)</title>
          </info>
          <para>Exclusive or, written as
	  <computeroutput>xor</computeroutput> is a special case of
	  <computeroutput>or</computeroutput> where the output is true
	  if one, and <emphasis>only</emphasis> one, of the inputs is
	  true.  This operation can surprisingly do many interesting
	  tricks, but you will not see a lot of it in the
	  kernel.</para>
          <table>
            <info>
              <title>Truth table for <emphasis>xor</emphasis></title>
            </info>
            <tgroup cols="3">
              <thead>
                <row>
                  <entry>Input 1</entry>
                  <entry>Input 2</entry>
                  <entry>Output</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                </row>
                <row>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>1</computeroutput>
                  </entry>
                  <entry>
                    <computeroutput>0</computeroutput>
                  </entry>
                </row>
              </tbody>
            </tgroup>
          </table>
        </section>
      </section>
      <section>
        <info>
          <title>How computers use boolean operations</title>
        </info>
        <para>Believe it or not, essentially everything your computer
	does comes back to the above operations.  For example, the
	half adder is a type of circuit made up from boolean
	operations that can add bits together (it is called a half
	adder because it does not handle carry bits).  Put more half
	adders together, and you will start to build something that
	can add together long binary numbers.  Add some external
	memory, and you have a computer.
	</para>
        <para>Electronically, the boolean operations are implemented
	in <emphasis>gates</emphasis> made by
	<emphasis>transistors</emphasis>.  This is why you might have
	heard about transistor counts and things like Moore's Law.  The
	more transistors, the more gates, the more things you can add
	together.  To create the modern computer, there are an awful
	lot of gates, and an awful lot of transistors.  Some of the
	latest Itanium processors have around 460 million
	transistors.</para>
      </section>
      <section>
        <info>
          <title>Working with binary in C</title>
        </info>
        <para>In C we have a direct interface to all of the above
	operations.  The following table describes the
	operators</para>
        <table>
          <info>
            <title>Boolean operations in C</title>
          </info>
          <tgroup cols="2">
            <thead>
              <row>
                <entry>Operation</entry>
                <entry>Usage in C</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <computeroutput>not</computeroutput>
                </entry>
                <entry>
                  <computeroutput>!</computeroutput>
                </entry>
              </row>
              <row>
                <entry>
                  <computeroutput>and</computeroutput>
                </entry>
                <entry>
                  <computeroutput>&amp;</computeroutput>
                </entry>
              </row>
              <row>
                <entry>
                  <computeroutput>or</computeroutput>
                </entry>
                <entry>
                  <computeroutput>|</computeroutput>
                </entry>
              </row>
              <row>
                <entry>
                  <computeroutput>xor</computeroutput>
                </entry>
                <entry>
                  <computeroutput>^</computeroutput>
                </entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>We use these operations on variables to modify the bits
	within the variable.  Before we see examples of this, first we
	must divert to describe hexadecimal notation.</para>
      </section>
    </section>
    <section>
      <info>
        <title>Hexadecimal</title>
      </info>
      <para>Hexadecimal refers to a base 16 number system.  We use
      this in computer science for only one reason, it makes it easy
      for humans to think about binary numbers.  Computers only ever
      deal in binary and hexadecimal is simply a shortcut for us
      humans trying to work with the computer.</para>
      <para>So why base 16?  Well, the most natural choice is base 10,
      since we are used to thinking in base 10 from our every day
      number system.  But base 10 does not work well with binary -- to
      represent 10 different elements in binary, we need four bits.
      Four bits, however, gives us sixteen possible combinations.  So
      we can either take the very tricky road of trying to convert
      between base 10 and binary, or take the easy road and make up a
      base 16 number system -- hexadecimal!</para>
      <para>Hexadecimal uses the standard base 10 numerals, but adds
      <computeroutput>A B C D E F</computeroutput> which refer to
      <computeroutput>10 11 12 13 14 15</computeroutput> (n.b. we
      start from zero).</para>
      <para>Traditionally, any time you see a number prefixed by
      <computeroutput>0x</computeroutput> this will denote a
      hexadecimal number.</para>
      <para>As mentioned, to represent 16 different patterns in
      binary, we would need exactly four bits.  Therefore, each
      hexadecimal numeral represents exactly four bits.  You should
      consider it an exercise to learn the following table off by
      heart.</para>
      <table>
        <info>
          <title>Hexadecimal, Binary and Decimal</title>
        </info>
        <tgroup cols="3">
          <thead>
            <row>
              <entry>Hexadecimal</entry>
              <entry>Binary</entry>
              <entry>Decimal</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <computeroutput>0</computeroutput>
              </entry>
              <entry>
                <computeroutput>0000</computeroutput>
              </entry>
              <entry>
                <computeroutput>0</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>1</computeroutput>
              </entry>
              <entry>
                <computeroutput>0001</computeroutput>
              </entry>
              <entry>
                <computeroutput>1</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>2</computeroutput>
              </entry>
              <entry>
                <computeroutput>0010</computeroutput>
              </entry>
              <entry>
                <computeroutput>2</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>3</computeroutput>
              </entry>
              <entry>
                <computeroutput>0011</computeroutput>
              </entry>
              <entry>
                <computeroutput>3</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>4</computeroutput>
              </entry>
              <entry>
                <computeroutput>0100</computeroutput>
              </entry>
              <entry>
                <computeroutput>4</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>5</computeroutput>
              </entry>
              <entry>
                <computeroutput>0101</computeroutput>
              </entry>
              <entry>
                <computeroutput>5</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>6</computeroutput>
              </entry>
              <entry>
                <computeroutput>0110</computeroutput>
              </entry>
              <entry>
                <computeroutput>6</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>7</computeroutput>
              </entry>
              <entry>
                <computeroutput>0111</computeroutput>
              </entry>
              <entry>
                <computeroutput>7</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>8</computeroutput>
              </entry>
              <entry>
                <computeroutput>1000</computeroutput>
              </entry>
              <entry>
                <computeroutput>8</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>9</computeroutput>
              </entry>
              <entry>
                <computeroutput>1001</computeroutput>
              </entry>
              <entry>
                <computeroutput>9</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>A</computeroutput>
              </entry>
              <entry>
                <computeroutput>1010</computeroutput>
              </entry>
              <entry>
                <computeroutput>10</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>B</computeroutput>
              </entry>
              <entry>
                <computeroutput>1011</computeroutput>
              </entry>
              <entry>
                <computeroutput>11</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>C</computeroutput>
              </entry>
              <entry>
                <computeroutput>1100</computeroutput>
              </entry>
              <entry>
                <computeroutput>12</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>D</computeroutput>
              </entry>
              <entry>
                <computeroutput>1101</computeroutput>
              </entry>
              <entry>
                <computeroutput>13</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>E</computeroutput>
              </entry>
              <entry>
                <computeroutput>1110</computeroutput>
              </entry>
              <entry>
                <computeroutput>14</computeroutput>
              </entry>
            </row>
            <row>
              <entry>
                <computeroutput>F</computeroutput>
              </entry>
              <entry>
                <computeroutput>1111</computeroutput>
              </entry>
              <entry>
                <computeroutput>15</computeroutput>
              </entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <para>Of course there is no reason not to continue the pattern
      (say, assign G to the value 16), but 16 values is an excellent
      trade off between the vagaries of human memory and the number of
      bits used by a computer (occasionally you will also see base 8
      used, for example for file permissions under UNIX).  We simply
      represent larger numbers of bits with more numerals.  For
      example, a sixteen bit variable can be represented by 0xAB12,
      and to find it in binary simply take each individual numeral,
      convert it as per the table and join them all together (so
      <computeroutput>0xAB12</computeroutput> ends up as the 16-bit
      binary number
      <computeroutput>1010101100010010</computeroutput>).  We can use
      the reverse to convert from binary back to hexadecimal.</para>
      <para>We can also use the same repeated division scheme to
      change the base of a number.  For example, to find 203 in
      hexadecimal</para>
      <table>
        <info>
          <title>Convert 203 to hexadecimal</title>
        </info>
        <tgroup cols="4">
          <thead>
            <row>
              <entry>Quotient</entry>
              <entry/>
              <entry>Remainder</entry>
              <entry/>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>203<subscript>10</subscript> &#xF7; 16 =</entry>
              <entry>12</entry>
              <entry>11 (0xB)</entry>
              <entry/>
            </row>
            <row>
              <entry>12<subscript>10</subscript> &#xF7; 16 =</entry>
              <entry>0</entry>
              <entry>12 (0xC)</entry>
              <entry>&#x2191;</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <para>Hence 203 in hexadecimal is
      <computeroutput>0xCB</computeroutput>.</para>
    </section>
    <section>
      <info>
        <title>Practical Implications</title>
      </info>
      <section>
        <info>
          <title>Use of binary in code</title>
        </info>
        <para>Whilst binary is the underlying language of every
      computer, it is entirely practical to program a computer in high
      level languages without knowing the first thing about it.
      However, for the low level code we are interested in a few
      fundamental binary principles are used repeatedly.</para>
      </section>
      <section>
        <info>
          <title>Masking and Flags</title>
        </info>
        <section>
          <info>
            <title>Masking</title>
          </info>
          <para>In low level code, it is often important to keep your
	structures and variables as space efficient as possible.  In
	some cases, this can involve effectively packing two
	(generally related) variables into one.</para>
          <para>Remember each bit represents two states, so if we know a
	variable only has, say, 16 possible states it can be
	represented by 4 bits (i.e. 2<superscript>4</superscript>=16
	unique values).  But the smallest type we can declare in C is
	8 bits (a <computeroutput>char</computeroutput>), so we can
	either waste four bits, or find some way to use those left
	over bits.</para>
          <para>We can easily do this by the process of
	<emphasis>masking</emphasis>.  Remembering the rules of the
	logical operations, it should become clear how the values are
	extracted.</para>
          <para>The process is illustrated in the figure below.  We are
	interested in the lower four bits, so set our mask to have
	these bits set to <computeroutput>1</computeroutput>.  Since
	the <computeroutput>logical and</computeroutput> operation
	will only set the bit if <emphasis>both</emphasis> bits are
	<computeroutput>1</computeroutput>, those bits of the mask set
	to <computeroutput>0</computeroutput> effectively hide the bits
	we are not interested in.</para>
          <figure>
            <info>
              <title>Masking</title>
            </info>
            <mediaobject>
              <imageobject>
                <imagedata fileref="chapter01/figures/masking.eps" format="EPS"/>
              </imageobject>
              <imageobject role="fo">
                <imagedata fileref="figures/masking.svg" format="SVG" scalefit="1" width="100%" contentdepth="100%"/>
              </imageobject>
              <imageobject role="html">
                <imagedata fileref="chapter01/figures/masking.png" format="PNG"/>
              </imageobject>
              <textobject>
                <phrase>By using a <emphasis>mask</emphasis> consisting
	      of all <computeroutput>1</computeroutput>'s and the
	      <computeroutput>logical and</computeroutput> operation,
	      we can extract only the bits we are interested
	      in.</phrase>
              </textobject>
            </mediaobject>
          </figure>
          <para>To get the top (blue) four bits, we would invert the
	mask.  You will note this gives a result of
	<computeroutput>0x90</computeroutput> when really we want a
	value of <computeroutput>0x09</computeroutput>.  To get the
	bits into the right position we use the <computeroutput>right
	shift</computeroutput> operation.</para>
          <para><emphasis>Setting</emphasis> the bits requires the
	<computeroutput>logical or</computeroutput> operation.
	However, rather than using
	<computeroutput>1</computeroutput>'s as the mask, we use
	<computeroutput>0</computeroutput>'s.  You should draw a
	diagram similar to the above figure and work through setting
	bits with the <computeroutput>logical
	or</computeroutput> operation.</para>
        </section>
        <section>
          <info>
            <title>Flags</title>
          </info>
          <para>Often a program will have a large number of variables
	that only exist as <emphasis>flags</emphasis> to some
	condition.  For example, a state machine is an algorithm that
	transitions through a number of different states but may only
	be in one at a time.  Say it has 8 different states; we could
	easily declare 8 different variables, one for each state.  But
	in many cases it is better to declare <emphasis>one 8 bit
	variable</emphasis> and assign each bit to
	<emphasis>flag</emphasis> flag a particular state.</para>
          <para>Flags are a special case of masking, but each bit
	represents a particular boolean state (on or off).  An
	<emphasis>n</emphasis> bit variable can hold
	<emphasis>n</emphasis> different flags.  See the code example
	below for a typical example of using flags -- you will see
	variations on this basic code very often.
	</para>
          <example xml:id="flag-code">
            <info>
              <title>Using flags</title>
            </info>
            <programlisting language="c" linenumbering="numbered">
              <xi:include href="code/flags.c" parse="text"/>
            </programlisting>
          </example>
        </section>
      </section>
    </section>
  </section>
  <section xml:id="types">
    <info>
      <title>Types and Number Representation</title>
    </info>
    <section>
      <info>
        <title>C Standards</title>
      </info>
      <para>Although a slight divergence, it is important to
      understand a bit of history about the C language.</para>
      <para>C is the common languge of the systems programming world.
      Every operating system and its associated system libraries in
      common use is written in C, and every system provides a C
      compiler.  To stop the language diverging across each of these
      systems where each would be sure to make numerous incompatible
      changes, a strict standard has been written for the
      language.</para>
      <para>Officially this standard is known as ISO/IEC 9899:1999(E),
      but is more commonly referred to by its shortened name
      <emphasis>C99</emphasis>.  The standard is maintained by the
      International Standards Organisation (ISO) and the full standard
      is available for purchase online.  Older standards versions such
      as C89 (the predecessor to C99 released in 1989) and ANSI C are
      no longer in common usage and are encompassed within the latest
      standard.  The standard documentation is very technical, and
      details most every part of the language.  For example it
      explains the syntax (in Backus Naur form), standard
      <computeroutput>#define</computeroutput> values and how
      operations should behave.</para>
      <para>It is also important to note what the C standards does
      <emphasis>not</emphasis> define.  Most importantly the standard
      needs to be appropriate for every architecture, both present and
      future.  Consequently it takes care <emphasis>not</emphasis> to
      define areas that are architecture dependent.  The "glue"
      between the C standard and the underlying architecture is the
      Application Binary Interface (or ABI) which we discuss below.
      In several places the standard will mention that a particular
      operation or construct has an unspecified or implementation
      dependent result.  Obviously the programmer can not depend on
      these outcomes if they are to write portable code.</para>
      <section>
        <info>
          <title>GNU C</title>
        </info>
        <para>The GNU C Compiler, more commonly referred to as
	<application>gcc</application>, almost completely implements
	the C99 standard.  However it also implements a range of
	extensions to the standard which programmers will often use to
	gain extra functionality, at the expense of portability to
	another compiler.  These extensions are usually related to
	very low level code and are much more common in the system
	programming field; the most common extension being used in
	this area being inline assembly code.  Programmers should read
	the <application>gcc</application> documentation and
	understand when they may be using features that diverge from
	the standard.</para>
        <para><application>gcc</application> can be directed to adhere
	strictly to the standard (the
	<computeroutput>-std=c99</computeroutput> flag for example)
	and warn or create an error when certain things are done that
	are not in the standard.  This is obviously appropriate if you
	need to ensure that you can move your code easily to another
	compiler.</para>
      </section>
    </section>
    <section>
      <info>
        <title>Types</title>
      </info>
      <para>As programmers, we are familiar with using variables to
      represent an area of memory to hold a value.  In a
      <emphasis>typed</emphasis> language, such as C, every variable
      must be declared with a <emphasis>type</emphasis>.  The type
      tells the compiler about what we expect to store in a variable;
      the compiler can then both allocate sufficient space for this
      usage and check that the programmer does not violate the rules
      of the type.  In the example below, we see an example of the
      space allocated for some common types of variables.</para>
      <figure>
        <info>
          <title>Types</title>
        </info>
        <mediaobject>
          <imageobject>
            <imagedata fileref="chapter01/figures/types.eps" format="EPS"/>
          </imageobject>
          <imageobject role="fo">
            <imagedata fileref="figures/types.svg" format="SVG" scalefit="1" width="100%" contentdepth="100%"/>
          </imageobject>
          <imageobject role="html">
            <imagedata fileref="chapter01/figures/types.png" format="PNG"/>
          </imageobject>
          <textobject>
            <phrase>The processor sees memory only a row of bytes.
		Adding types to variables helps the compiler ensure
		that code is acting correctly.  Above illustrates some
		common types, and how they map to memory.</phrase>
          </textobject>
        </mediaobject>
      </figure>
      <para>The C99 standard purposely only mentions the
      <emphasis>smallest</emphasis> possible size of each of the types
      defined for C.  This is because across different processor
      architectures and operating systems the best size for
      types can be wildly different.</para>
      <para>To be completely safe programmers need to never assume the
      size of any of their variables, however a functioning system
      obviously needs agreements on what sizes types are going to be
      used in the system.  Each architecture and operating system
      conforms to an <emphasis>Application Binary Interface</emphasis>
      or <emphasis>ABI</emphasis>.  The ABI for a system fills in the
      details between the C standard and the requirements of the
      underlying hardware and operating system.  An ABI is written for
      a specific processor and operating system combination.</para>
      <table>
        <info>
          <title>Standard Integer Types and Sizes</title>
        </info>
        <tgroup cols="3">
          <thead>
            <row>
              <entry>Type</entry>
              <entry>C99 minimum size (bits)</entry>
              <entry>Common size (32 bit architecture)</entry>
            </row>
          </thead>
          <tbody>
            <row>
              <entry>
                <computeroutput>char</computeroutput>
              </entry>
              <entry>8</entry>
              <entry>8</entry>
            </row>
            <row>
              <entry>
                <computeroutput>short</computeroutput>
              </entry>
              <entry>16</entry>
              <entry>16</entry>
            </row>
            <row>
              <entry>
                <computeroutput>int</computeroutput>
              </entry>
              <entry>16</entry>
              <entry>32</entry>
            </row>
            <row>
              <entry>
                <computeroutput>long</computeroutput>
              </entry>
              <entry>32</entry>
              <entry>32</entry>
            </row>
            <row>
              <entry>
                <computeroutput>long long</computeroutput>
              </entry>
              <entry>64</entry>
              <entry>64</entry>
            </row>
            <row>
              <entry>Pointers</entry>
              <entry>Implementation dependent</entry>
              <entry>32</entry>
            </row>
          </tbody>
        </tgroup>
      </table>
      <para>Above we can see the only divergence from the standard is
      that <computeroutput>int</computeroutput> is commonly a 32 bit
      quantity, which is twice the strict minimum 16 bit size that the
      C99 requires.</para>
      <para>Pointers are really just an address (i.e. their value is
      an address and thus "points" somewhere else in memory) therefore
      a pointer needs to be sufficient in size to be able to address
      any memory in the system.</para>
      <section>
        <info>
          <title>64 bit</title>
        </info>
        <para>One area that causes confusion is the introduction of 64
	bit computing.  This means that the processor can handle
	addresses 64 bits in length (specifically the registers are 64
	bits wide; a topic we discuss in <xref linkend="chapter02"/>).</para>
        <para>This firstly means that all pointers are required to be
	a 64 bits wide so they can represent any possible address in
	the system.  However, system implementers must then make
	decisions about the size of the other types.  Two common
	models are widely used, as shown below.</para>
        <table>
          <info>
            <title>Standard Scalar Types and Sizes</title>
          </info>
          <tgroup cols="4">
            <thead>
              <row>
                <entry>Type</entry>
                <entry>C99 minimum size (bits)</entry>
                <entry>Common size (LP64)</entry>
                <entry>Common size (Windows)</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>
                  <computeroutput>char</computeroutput>
                </entry>
                <entry>8</entry>
                <entry>8</entry>
                <entry>8</entry>
              </row>
              <row>
                <entry>
                  <computeroutput>short</computeroutput>
                </entry>
                <entry>16</entry>
                <entry>16</entry>
                <entry>16</entry>
              </row>
              <row>
                <entry>
                  <computeroutput>int</computeroutput>
                </entry>
                <entry>16</entry>
                <entry>32</entry>
                <entry>32</entry>
              </row>
              <row>
                <entry>
                  <computeroutput>long</computeroutput>
                </entry>
                <entry>32</entry>
                <entry>64</entry>
                <entry>32</entry>
              </row>
              <row>
                <entry>
                  <computeroutput>long long</computeroutput>
                </entry>
                <entry>64</entry>
                <entry>64</entry>
                <entry>64</entry>
              </row>
              <row>
                <entry>Pointers</entry>
                <entry>Implementation dependent</entry>
                <entry>64</entry>
                <entry>64</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>You can see that in the LP64 (long-pointer 64) model
	<computeroutput>long</computeroutput> values are defined to be
	64 bits wide.  This is different to the 32 bit model we showed
	previously.  The LP64 model is widely used on UNIX
	systems.</para>
        <para>In the other model,
	<computeroutput>long</computeroutput> remains a 32 bit value.
	This maintains maximum compatibility with 32 code.  This model
	is in use with 64 bit Windows.</para>
        <para>There are good reasons why the size of
	<computeroutput>int</computeroutput> was not increased to 64
	bits in either model.  Consider that if the size of
	<computeroutput>int</computeroutput> is increased to 64 bits
	you leave programmers no way to obtain a 32 bit variable.  The
	only possibly is redefining
	<computeroutput>shorts</computeroutput> to be a larger 32 bit
	type.</para>
        <para>A 64 bit variable is so large that it is not generally
	required to represent many variables.  For example, loops very
	rarely repeat more times than would fit in a 32 bit variable
	(4294967296 times!).  Images usually are usually represented
	with 8 bits for each of a red, green and blue value and an
	extra 8 bits for extra (alpha channel) information; a total of
	32 bits.  Consequently for many cases, using a 64 bit variable
	will be wasting at least the top 32 bits (if not more).  Not
	only this, but the size of an integer array has now doubled
	too.  This means programs take up more system memory (and thus
	more cache; discussed in detail in <xref linkend="chapter02"/>) for no real improvement.  For
	the same reason Windows elected to keep their long values as
	32 bits; since much of the Windows API was originally written
	to use long variables on a 32 bit system and hence does not
	require the extra bits this saves considerable wasted space in
	the system without having to re-write all the API.</para>
        <para>If we consider the proposed alternative where
	<computeroutput>short</computeroutput> was redefined to be a
	32 bit variable; programmers working on a 64 bit system could
	use it for variables they know are bounded to smaller values.
	However, when moving back to a 32 bit system their same
	<computeroutput>short</computeroutput> variable would now be
	only 16 bits long, a value which is much more realistically
	overflowed (65536).</para>
        <para>By making a programmer request larger variables when
	they know they will be needed strikes a balance with respect
	to portability concerns and wasting space in binaries.</para>
      </section>
      <section>
        <info>
          <title>Type qualifiers</title>
        </info>
        <para>The C standard also talks about some qualifiers for
	variable types.  For example
	<computeroutput>const</computeroutput> means that a variable
	will never be modified from its original value and
	<computeroutput>volatile</computeroutput> suggests to the
	compiler that this value might change outside program
	execution flow so the compiler must be careful not to re-order
	access to it in any way.</para>
        <para><computeroutput>signed</computeroutput> and
	<computeroutput>unsigned</computeroutput> are probably the two
	most important qualifiers; and they say if a variable can take
	on a negative value or not.  We examine this in more detail
	below.</para>
        <para>Qualifiers are all intended to pass extra information
	about how the variable will be used to the compiler.  This
	means two things; the compiler can check if you are violating
	your own rules (e.g. writing to a
	<computeroutput>const</computeroutput> value) and it can make
	optimisations based upon the extra knowledge (examined in
	later chapters).</para>
      </section>
      <section>
        <info>
          <title>Standard Types</title>
        </info>
        <para>C99 realises that all these rules, sizes and portability
	concerns can become very confusing very quickly.  To help, it
	provides a series of special types which can specify the exact
	properties of a variable.  These are defined in
	<computeroutput>&lt;stdint.h&gt;</computeroutput> and have the
	form <computeroutput>qtypes_t</computeroutput> where
	<computeroutput>q</computeroutput> is a qualifier,
	<computeroutput>type</computeroutput> is the base type,
	<computeroutput>s</computeroutput> is the width in bits and
	<computeroutput>_t</computeroutput> is an extension so you
	know you are using the C99 defined types.</para>
        <para>So for example <computeroutput>uint8_t</computeroutput>
	is an unsigned integer exactly 8 bits wide.  Many other types
	are defined; the complete list is detailed in C99 17.8 or
	(more cryptically) in the header file. <footnote><para>Note
	that C99 also has portability helpers for
	<computeroutput>printf</computeroutput>.  The
	<computeroutput>PRI</computeroutput> macros in
	<computeroutput>&lt;inttypes.h&gt;</computeroutput> can be
	used as specifiers for types of specified sizes.  Again see
	the standard or pull apart the headers for full
	information.</para></footnote></para>
        <para>It is up to the system implementing the C99 standard to
	provide these types for you by mapping them to appropriate
	sized types on the target system; on Linux these headers are
	provided by the system libraries.</para>
      </section>
      <section>
        <info>
          <title>Types in action</title>
        </info>
        <para>Below in <xref linkend="type-warnings" /> we see an
        example of how types place restrictions on what operations are
        valid for a variable, and how the compiler can use this
        information to warn when variables are used in an incorrect
        fashion.  In this code, we firstly assign an integer value
        into a <computeroutput>char</computeroutput> variable.  Since
        the <computeroutput>char</computeroutput> variable is smaller,
        we loose the correct value of the integer.  Further down, we
        attempt to assign a pointer to a
        <computeroutput>char</computeroutput> to memory we designated
        as an <computeroutput>integer</computeroutput>.  This
        operation can be done; but it is not safe.  The first example
        is run on a 32-bit Pentium machine, and the correct value is
        returned.  However, as shown in the second example, on a
        64-bit Itanium machine a pointer is 64 bits (8 bytes) long,
        but an integer is only 4 bytes long.  Clearly, 8 bytes can not
        fit into 4!  We can attempt to "fool" the compiler by
        <emphasis>casting</emphasis> the value before assigning it;
        note that in this case we have shot ourselves in the foot by
        doing this cast and ignoring the compiler warning since the
        smaller variable can not hold all the information from the
        pointer and we end up with an invalid address.</para>
        <example xml:id="type-warnings">
          <info>
            <title>Example of warnings when types are not matched</title>
          </info>
          <programlisting linenumbering="numbered" language="c"><xi:include href="code/types.c" parse="text" /></programlisting>
          <programlisting linenumbering="numbered"><xi:include href="code/type-warnings.txt" parse="text"/></programlisting>
        </example>
      </section>
    </section>
    <section>
      <info>
        <title>Number Representation</title>
      </info>
      <section>
        <info>
          <title>Negative Values</title>
        </info>
        <para>With our modern base 10 numeral system we indicate a
	negative number by placing a minus
	(<computeroutput>-</computeroutput>) sign in front of it.
	When using binary we need to use a different system to
	indicate negative numbers.</para>
        <para>There is only one scheme in common use on modern
	hardware, but C99 defines three acceptable methods for
	negative value representation.</para>
        <section>
          <info>
            <title>Sign Bit</title>
          </info>
          <para>The most straight forward method is to simply say that
	  one bit of the number indicates either a negative or
	  positive value depending on it being set or not.</para>
          <para>This is analogous to mathematical approach of having a
	  <computeroutput>+</computeroutput> and
	  <computeroutput>-</computeroutput>.  This is fairly logical,
	  and some of the original computers did represent negative
	  numbers in this way.  But using binary numbers opens up some
	  other possibilities which make the life of hardware designers
	  easier.</para>
          <para>However, notice that the value
	  <computeroutput>0</computeroutput> now has two equivalent
	  values; one with the sign bit set and one without.
	  Sometimes these values are referred to as
	  <computeroutput>+0</computeroutput> and
	  <computeroutput>-0</computeroutput> respectively.</para>
        </section>
        <section>
          <info>
            <title>One's Complement</title>
          </info>
          <para>One's complement simply applies the
	  <emphasis>not</emphasis> operation to the positive number to
	  represent the negative number.  So, for example the value
	  -90 (-0x5A) is represented by <computeroutput>~01011010 =
	  10100101</computeroutput><footnote><para>The
	  <computeroutput>~</computeroutput> operator is the C
	  language operator to apply
	  <computeroutput>NOT</computeroutput> to the value.  It is
	  also occasionally called the one's complement operator, for
	  obvious reasons now!</para></footnote></para>
          <para>With this scheme the biggest advantage is that to add
	  a negative number to a positive number no special logic is
	  required, except that any additional carry left over must be
	  added back to the final value.  Consider</para>
          <table>
            <info>
              <title>One's Complement Addition</title>
            </info>
            <tgroup cols="3">
              <thead>
                <row>
                  <entry>Decimal</entry>
                  <entry>Binary</entry>
                  <entry>Op</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>-90</entry>
                  <entry>10100101</entry>
                  <entry>+</entry>
                </row>
                <row>
                  <entry>100</entry>
                  <entry>01100100</entry>
                  <entry/>
                </row>
                <row>
                  <entry>---</entry>
                  <entry>--------</entry>
                  <entry/>
                </row>
                <row>
                  <entry>10</entry>
                  <entry><superscript>1</superscript>00001001</entry>
                  <entry>9</entry>
                </row>
                <row>
                  <entry/>
                  <entry>00001010</entry>
                  <entry>10</entry>
                </row>
              </tbody>
            </tgroup>
          </table>
          <para>If you add the bits one by one, you find you end up
	  with a carry bit at the end (highlighted above).  By adding
	  this back to the original we end up with the correct value,
	  10</para>
          <para>Again we still have the problem with two zeros being
	  represented.  Again no modern computer uses one's
	  complement, mostly because there is a better scheme.</para>
        </section>
        <section>
          <info>
            <title>Two's Complement</title>
          </info>
          <para>Two's complement is just like one's complement, except
	  the negative representation has <emphasis>one</emphasis>
	  added to it and we discard any left over carry bit.  So to
	  continue with the example from before,
	  <computeroutput>-90</computeroutput> would be
	  <computeroutput>~01011010+1=10100101+1 =
	  10100110</computeroutput>.</para>
          <para>This means there is a slightly odd symmetry in the
	  numbers that can be represented; for example with an 8 bit
	  integer we have
	  <computeroutput>2^<superscript>8</superscript> =
	  256</computeroutput> possible values; with our sign bit
	  representation we could represent -127 thru 127 but with
	  two's complement we can represent -127 thru 128.  This is
	  because we have removed the problem of having two zeros;
	  consider that "negative zero" is <computeroutput>(~00000000
	 +1)=(11111111+1)=00000000</computeroutput> (note
	  discarded carry bit).</para>
          <table>
            <info>
              <title>Two's Complement Addition</title>
            </info>
            <tgroup cols="3">
              <thead>
                <row>
                  <entry>Decimal</entry>
                  <entry>Binary</entry>
                  <entry>Op</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>-90</entry>
                  <entry>10100110</entry>
                  <entry>+</entry>
                </row>
                <row>
                  <entry>100</entry>
                  <entry>01100100</entry>
                  <entry/>
                </row>
                <row>
                  <entry>---</entry>
                  <entry>--------</entry>
                  <entry/>
                </row>
                <row>
                  <entry>10</entry>
                  <entry>00001010</entry>
                  <entry/>
                </row>
              </tbody>
            </tgroup>
          </table>
          <para>You can see that by implementing two's complement
	  hardware designers need only provide logic for addition
	  circuits; subtraction can be done by two's complement
	  negating the value to be subtracted and then adding the new
	  value.</para>
          <para>Similarly you could implement multiplication with
	  repeated addition and division with repeated subtraction.
	  Consequently two's complement can reduce all simple
	  mathematical operations down to addition!</para>
          <para>All modern computers use two's complement
	  representation.</para>
          <section xml:id="sign_extension">
            <info>
              <title>Sign-extension</title>
            </info>
            <para>Because of two's complement format, when increasing
	      the size of signed value, it is important that the
	      additional bits be <emphasis>sign-extended</emphasis>;
	      that is, copied from the top-bit of the existing
	      value.</para>
            <para>For example, the value of an 32-bit
	    <computeroutput>int</computeroutput>
	    <computeroutput>-10</computeroutput> would be represented
	    in two's complement binary as
	    <computeroutput>11111111111111111111111111110110</computeroutput>.
	    If one were to cast this to a 64-bit <computeroutput>long
	    long int</computeroutput>, we would need to ensure that
	    the additional 32-bits were set to
	    <computeroutput>1</computeroutput> to maintain the same
	    sign as the original.</para>
            <para>Thanks to two's complement, it is sufficient to take
            the top bit of the exiting value and replace all the added
            bits with this value.  This processes is referred to as
            <emphasis>sign-extension</emphasis> and is usually handled
            by the compiler in situations as defined by the language
            standard, with the processor generally providing special
            instructions to take a value and sign-extended it to some
            larger value.</para>
          </section>
        </section>
      </section>
      <section>
        <info>
          <title>Floating Point</title>
        </info>
        <para>So far we have only discussed integer or whole numbers;
	the class of numbers that can represent decimal values is
	called <emphasis>floating point</emphasis>.</para>
        <para>To create a decimal number, we require some way to
	represent the concept of the decimal place in binary.  The
	most common scheme for this is known as the <emphasis>IEEE-754
	floating point standard</emphasis> because the standard is
	published by the Institute of Electric and Electronics
	Engineers.  The scheme is conceptually quite simple and is
	somewhat analogous to "scientific notation".</para>
        <para>In scientific notation the value
	<computeroutput>123.45</computeroutput> might commonly be
	represented as
	<computeroutput>1.2345x10<superscript>2</superscript></computeroutput>.
	We call <computeroutput>1.2345</computeroutput> the
	<emphasis>mantissa</emphasis> or
	<emphasis>significand</emphasis>,
	<computeroutput>10</computeroutput> is the
	<emphasis>radix</emphasis> and
	<computeroutput>2</computeroutput> is the
	<emphasis>exponent</emphasis>.
	</para>
        <para>In the IEEE floating point model, we break up the
	available bits to represent the sign, mantissa and exponent of
	a decimal number.  A decimal number is represented by
	<computeroutput>sign &#xD7; significand &#xD7;
	2^<superscript>exponent</superscript></computeroutput>.</para>
        <para>The sign bit equates to either
	<computeroutput>1</computeroutput> or
	<computeroutput>-1</computeroutput>.  Since we are working in
	binary, we always have the implied radix of
	<computeroutput>2</computeroutput>.</para>
        <para>There are differing widths for a floating point value --
	we examine below at only a 32 bit value.  More bits allows
	greater precision.</para>
        <table>
          <info>
            <title>IEEE Floating Point</title>
          </info>
          <tgroup cols="3">
            <thead>
              <row>
                <entry>Sign</entry>
                <entry>Exponent</entry>
                <entry>Significand/Mantissa</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>S</entry>
                <entry>EEEEEEEE</entry>
                <entry>MMMMMMMMMMMMMMMMMMMMMMM</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>The other important factor is <emphasis>bias</emphasis>
	of the exponent.  The exponent needs to be able to represent
	both positive and negative values, thus an implied value of
	<computeroutput>127</computeroutput> is subtracted from the
	exponent.  For example, an exponent of
	<computeroutput>0</computeroutput> has an exponent field of
	<computeroutput>127</computeroutput>,
	<computeroutput>128</computeroutput> would represent
	<computeroutput>1</computeroutput> and
	<computeroutput>126</computeroutput> would represent
	<computeroutput>-1</computeroutput>.</para>
        <para>Each bit of the significand adds a little more precision
	to the values we can represent.  Consider the scientific
	notation representation of the value
	<computeroutput>198765</computeroutput>.  We could write this
	as
	<computeroutput>1.98765x10<superscript>6</superscript></computeroutput>,
	which corresponds to a representation below</para>
        <table>
          <info>
            <title>Scientific Notation for 1.98765x10^6</title>
          </info>
          <tgroup cols="7">
            <thead>
              <row>
                <entry>10<superscript>0</superscript></entry>
                <entry>.</entry>
                <entry>10<superscript>-1</superscript></entry>
                <entry>10<superscript>-2</superscript></entry>
                <entry>10<superscript>-3</superscript></entry>
                <entry>10<superscript>-4</superscript></entry>
                <entry>10<superscript>-5</superscript></entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>1</entry>
                <entry>.</entry>
                <entry>9</entry>
                <entry>8</entry>
                <entry>7</entry>
                <entry>6</entry>
                <entry>5</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>Each additional digit allows a greater range of decimal
	values we can represent.  In base 10, each digit after the
	decimal place increases the precision of our number by 10
	times.  For example, we can represent
	<computeroutput>0.0</computeroutput> through
	<computeroutput>0.9</computeroutput> (10 values) with one
	digit of decimal place, <computeroutput>0.00</computeroutput>
	through <computeroutput>0.99</computeroutput> (100 values)
	with two digits, and so on.  In binary, rather than each
	additional digit giving us 10 times the precision, we only get
	two times the precision, as illustrated in the table below.
	This means that our binary representation does not always map
	in a straight-forward manner to a decimal
	representation.</para>
        <table>
          <info>
            <title>Significands in binary</title>
          </info>
          <tgroup cols="7">
            <thead>
              <row>
                <entry>2<superscript>0</superscript></entry>
                <entry>.</entry>
                <entry>2<superscript>-1</superscript></entry>
                <entry>2<superscript>-2</superscript></entry>
                <entry>2<superscript>-3</superscript></entry>
                <entry>2<superscript>-4</superscript></entry>
                <entry>2<superscript>-5</superscript></entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>1</entry>
                <entry>.</entry>
                <entry>1/2</entry>
                <entry>1/4</entry>
                <entry>1/8</entry>
                <entry>1/16</entry>
                <entry>1/32</entry>
              </row>
              <row>
                <entry>1</entry>
                <entry>.</entry>
                <entry>0.5</entry>
                <entry>0.25</entry>
                <entry>0.125</entry>
                <entry>0.0625</entry>
                <entry>0.03125</entry>
              </row>
            </tbody>
          </tgroup>
        </table>
        <para>With only one bit of precision, our fractional precision
	is not very big; we can only say that the fraction is either
	<computeroutput>0</computeroutput> or
	<computeroutput>0.5</computeroutput>.  If we add another bit
	of precision, we can now say that the decimal value is one of
	either <computeroutput>0,0.25,0.5,0.75</computeroutput>.  With
	another bit of precision we can now represent the values
	<computeroutput>0,0.125,0.25,0.375,0.5,0.625,0.75,0.875</computeroutput>.</para>
        <para>Increasing the number of bits therefore allows us
	greater and greater precision.  However, since the range of
	possible numbers is infinite we will never have enough bits to
	represent <emphasis>any</emphasis> possible value.</para>
        <para>For example, if we only have two bits of precision and
	need to represent the value
	<computeroutput>0.3</computeroutput> we can only say that it
	is closest to <computeroutput>0.25</computeroutput>; obviously
	this is insufficient for most any application.  With 22 bits
	of significand we have a much finer resolution, but it is
	still not enough for most applications.  A
	<computeroutput>double</computeroutput> value increases the
	number of significand bits to 52 (it also increases the range
	of exponent values too).  Some hardware has an 84-bit float,
	with a full 64 bits of significand.  64 bits allows a
	tremendous precision and should be suitable for all but the
	most demanding of applications (XXX is this sufficient to
	represent a length to less than the size of an atom?)</para>
        <example xml:id="float-v-double">
          <info>
            <title>Floats versus Doubles</title>
          </info>
          <programlisting linenumbering="numbered">
            <xi:include href="code/floatvdouble.txt" parse="text"/>
          </programlisting>
        </example>
        <para>A practical example is illustrated above.  Notice that
	for the default 6 decimal places of precision given by
	<computeroutput>printf</computeroutput> both answers are the
	same, since they are rounded up correctly.  However, when
	asked to give the results to a larger precision, in this case
	20 decimal places, we can see the results start to diverge.
	The code using <computeroutput>doubles</computeroutput> has a
	more accurate result, but it is still not
	<emphasis>exactly</emphasis> correct.  We can also see that
	programmers not explicitly dealing with
	<computeroutput>float</computeroutput> values still have
	problems with precision of variables!</para>
        <section>
          <info>
            <title>Normalised Values</title>
          </info>
          <para>In scientific notation, we can represent a value in
	  many different ways.  For example,
	  <computeroutput>10023x10^<superscript>0</superscript> =
	  1002.3x10<superscript>1</superscript> =
	  100.23x10<superscript>2</superscript></computeroutput>.  We
	  thus define the <emphasis>normalised</emphasis> version as
	  the one where <computeroutput>1/radix &lt;= significand &lt;
	  1</computeroutput>.  In binary this ensures that the
	  leftmost bit of the significand is <emphasis>always
	  one</emphasis>.  Knowing this, we can gain an extra bit of
	  precision by having the standard say that the leftmost bit
	  being one is implied.</para>
          <table>
            <info>
              <title>Example of normalising 0.375</title>
            </info>
            <tgroup cols="9">
              <thead>
                <row>
                  <entry>2<superscript>0</superscript></entry>
                  <entry>.</entry>
                  <entry>2<superscript>-1</superscript></entry>
                  <entry>2<superscript>-2</superscript></entry>
                  <entry>2<superscript>-3</superscript></entry>
                  <entry>2<superscript>-4</superscript></entry>
                  <entry>2<superscript>-5</superscript></entry>
                  <entry>Exponent</entry>
                  <entry>Calculation</entry>
                </row>
              </thead>
              <tbody>
                <row>
                  <entry>0</entry>
                  <entry>.</entry>
                  <entry>0</entry>
                  <entry>1</entry>
                  <entry>1</entry>
                  <entry>0</entry>
                  <entry>0</entry>
                  <entry>2^<superscript>0</superscript></entry>
                  <entry>(0.25+0.125) &#xD7; 1 = 0.375 </entry>
                </row>
                <row>
                  <entry>0</entry>
                  <entry>.</entry>
                  <entry>1</entry>
                  <entry>1</entry>
                  <entry>0</entry>
                  <entry>0</entry>
                  <entry>0</entry>
                  <entry>2^<superscript>-1</superscript></entry>
                  <entry>(0.5+0.25)&#xD7;.5=0.375</entry>
                </row>
                <row>
                  <entry>1</entry>
                  <entry>.</entry>
                  <entry>1</entry>
                  <entry>0</entry>
                  <entry>0</entry>
                  <entry>0</entry>
                  <entry>0</entry>
                  <entry>2^<superscript>-2</superscript></entry>
                  <entry>(1+0.5)&#xD7;0.25=0.375</entry>
                </row>
              </tbody>
            </tgroup>
          </table>
          <para>As you can see above, we can make the value normalised
	  by moving the bits upwards as long as we compensate by
	  increasing the exponent.</para>
          <section>
            <info>
              <title>Normalisation Tricks</title>
            </info>
            <para>A common problem programmers face is finding the
	    first set bit in a bitfield.  Consider the bitfield
	    <computeroutput>0100</computeroutput>; from the right the
	    first set bit would be bit
	    <computeroutput>2</computeroutput> (starting from zero, as
	    is conventional).</para>
            <para>The standard way to find this value is to shift
	    right, check if the uppermost bit is a
	    <computeroutput>1</computeroutput> and either terminate or
	    repeat.  This is a slow process; if the bitfield is 64
	    bits long and only the very last bit is set, you must go
	    through all the preceding 63 bits!</para>
            <para>However, if this bitfield value were the signficand
	    of a floating point number and we were to normalise it,
	    the value of the exponent would tell us how many times it
	    was shifted.  The process of normalising a number is
	    generally built into the floating point hardware unit on
	    the processor, so operates very fast; usually much faster
	    than the repeated shift and check operations.</para>
            <para>The example program below illustrates two methods of
	    finding the first set bit on an Itanium processor.  The
	    Itanium, like most server processors, has support for an
	    80-bit <emphasis>extended</emphasis> floating point type,
	    with a 64-bit significand.  This means a
	    <computeroutput>unsigned long</computeroutput> neatly fits
	    into the significand of a <computeroutput>long
	    double</computeroutput>.  When the value is loaded it is
	    normalised, and and thus by reading the exponent value
	    (minus the 16 bit bias) we can see how far it was
	    shifted.</para>
            <example xml:id="ffs">
              <info>
                <title>Program to find first set bit</title>
              </info>
              <programlisting linenumbering="numbered" language="c">
                <xi:include href="code/ffs.c" parse="text"/>
              </programlisting>
            </example>
          </section>
        </section>
        <section>
          <info>
            <title>Bringing it together</title>
          </info>
          <para>In the example code below we extract the components of
	  a floating point number and print out the value it
	  represents.  This will only work for a 32 bit floating point
	  value in the IEEE format; however this is common for most
	  architectures with the
	  <computeroutput>float</computeroutput> type.</para>
          <example xml:id="float-examine">
            <info>
              <title>Examining Floats</title>
            </info>
            <programlisting linenumbering="numbered" language="c">
              <xi:include href="code/float.c" parse="text"/>
            </programlisting>
          </example>
          <para>Sample output of the value
	  <computeroutput>8.45</computeroutput>, which we previously
	  examined, is shown below.</para>
          <example xml:id="float-examine-example">
            <info>
              <title>Analysis of <computeroutput>8.45</computeroutput></title>
            </info>
            <programlisting>
$ ./float 8.45
8.450000 = 1 * (1/2^0 + 1/2^5 + 1/2^6 + 1/2^7 + 1/2^10 + 1/2^11 + 1/2^14 + 1/2^15 + 1/2^18 + 1/2^19 + 1/2^22 + 1/2^23) * 2^3
8.450000 = 1 * (1/1 + 1/32 + 1/64 + 1/128 + 1/1024 + 1/2048 + 1/16384 + 1/32768 + 1/262144 + 1/524288 + 1/4194304 + 1/8388608) * 2^3
8.450000 = 1 * 1.05624997616 * 8.000000
8.450000 = 8.44999980927
</programlisting>
          </example>
          <para>From this example, we get some idea of how the
	  inaccuracies creep into our floating point numbers.</para>
        </section>
      </section>
    </section>
  </section>
</chapter>
